{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Analysis Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My project is about analyzing online forum discussions related to the topic of adoptiom. To accomplish this task I gathered textual data from 2 popular subreddits with the aims of discussing adoption: r/Adoption and r/Adopted. The full and combine data set is available at this [link](https://uchicago.box.com/s/qhblnxta8j0b2nc6gexywqr81jgr9er2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in c:\\users\\ethan\\anaconda3\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in c:\\users\\ethan\\anaconda3\\lib\\site-packages (from tensorflow-addons) (2.13.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\ethan\\anaconda3\\lib\\site-packages (from tensorflow-addons) (23.1)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SparseDense' from 'tensorflow_addons.layers' (c:\\Users\\Ethan\\anaconda3\\Lib\\site-packages\\tensorflow_addons\\layers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout, Embedding, GlobalMaxPooling1D, LSTM\n\u001b[0;32m     16\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m pip install tensorflow-addons\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_addons\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDense\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'SparseDense' from 'tensorflow_addons.layers' (c:\\Users\\Ethan\\anaconda3\\Lib\\site-packages\\tensorflow_addons\\layers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import json\n",
    "import sklearn.decomposition\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score, confusion_matrix\n",
    "\n",
    "# ! pip install tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, GlobalMaxPooling1D, LSTM\n",
    "# ! pip install tensorflow-addons\n",
    "from tensorflow_addons.layers import SparseDense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors, LdaModel\n",
    "\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import wordcloud \n",
    "# ! python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# data viz\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap as LSC\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adoptee_labels = ['Adoptee (UK)',\n",
    " ' Adoptee of Closed Adoption',\n",
    " 'Domestic Infant Adoptee',\n",
    " 'Transracial Adoptee',\n",
    " 'Adoptee',\n",
    " 'International Adoptee',\n",
    " 'Baby Scoop Era Adoptee',\n",
    " 'Adult Adoptee (DIA)',\n",
    " 'Mentally ill adopted teen',\n",
    " 'adopted at birth',\n",
    " 'Failed Adoptee',\n",
    " 'Reunited Adoptee',\n",
    " 'Adoptee ‚ù§Ô∏è',\n",
    " 'Adopted at 2 from Ukraine to the USA',\n",
    " 'Teen Adoptee, open adoption',\n",
    " 'Adoptee and Birth Parent',\n",
    " 'in adoption limbo ...',\n",
    " 'adopted family divorcee, adopted by birth mom',\n",
    " 'adoptee in reunion',\n",
    " 'adoptee',\n",
    " 'Korean-American  Adoptee',\n",
    " 'Adoptee with 6 parents',\n",
    " 'Happily reunited adoptee',\n",
    " '32/M/adoptee/in-reunion',\n",
    " 'Adopted @ 6yo',\n",
    " '(Adoptee, 1973)',\n",
    " 'Adoptee, looking',\n",
    " 'Adoptee/Step Dad',\n",
    " 'Adoptee and Birthmother',\n",
    " 'Adoptee recently reunited',\n",
    " 'Adopted',\n",
    " 'Adoptee Moderator',\n",
    " 'Adopted, trad/closed, Ohio',\n",
    " 'Adoptee, Foster Mama',\n",
    " 'Adult Adoptee',\n",
    " 'Closed At-Birth Adoptee',\n",
    " 'Foster Adoptee',\n",
    " 'Reunited adoptee',\n",
    " 'adoptee, closed adoption, seeking reunification',\n",
    " 'Chinese Adoptee',\n",
    " 'late-discovery-adoptee',\n",
    " 'Korean Adoptee, Married, CF',\n",
    " 'Adopted at birth',\n",
    " 'Russian - Kiwi Adoptee',\n",
    " 'Not Quite Adopted',\n",
    " 'adopted',\n",
    " 'INFP: The Dreamer',\n",
    " 'Transracial adoptee',\n",
    " 'Closed Adoption Adoptee',\n",
    " 'Adoptee Recently Reuinited',\n",
    " 'transracial &amp; transnational adoptee',\n",
    " 'Transracial US Domestic Adult Adoptee',\n",
    " 'Korean Adoptee',\n",
    " 'LDA, FFY, Indigenous adoptee',\n",
    " 'Adoptee',\n",
    " 'Chinese Adoptee in Canada (23F)',\n",
    " '1970 Closed Adoption Adoptee',\n",
    " 'adopted from China at 12mo',\n",
    " 'Open Adoption Adoptee',\n",
    " 'LDA, FFY, Indigenous adoptee',\n",
    " 'Russian Adoptee',\n",
    " 'International adoptive mom of two (Vietnam)',\n",
    " 'Russian adoptee',\n",
    " 'neo city üíö',\n",
    " 'Pre-Adoptive / Prospective Parents (PAP)',\n",
    " 'Teen Adoptee',\n",
    " 'Live, Love, Learn',\n",
    " 'transracial adoptee',\n",
    " 'adoptee // 23',\n",
    " 'TRA/ICA/KAD (minor)',\n",
    " 'Adult Adopteeü§ç',\n",
    " 'Adult adoptee, hoping to adopt',\n",
    " 'Punjabi-Canto transracial adoptee',\n",
    " 'Asian Adoptee',\n",
    " 'adoptee',\n",
    " 'Adoptee From USA',\n",
    " 'recently found my bio fam :)',\n",
    " 'non paternal event / LDA',\n",
    " 'Adopted',\n",
    " 'Late Disclosure Adoptee, Future Adoptive Parent',\n",
    " 'Private Infant Adoptee - 24F',\n",
    " 'adopted at infancy',\n",
    " '(Lifelong Open) Adoptee',\n",
    " 'Adoptee (üá®üá≥ ‚Äî&gt; üá´üá∑)',\n",
    " 'Late Discovery Adoptee (LDA)',\n",
    " 'International Asian TRA',\n",
    " \"int'l adoptee\",\n",
    " 'TRA',\n",
    " 'Teen Adoptee, open adoption',\n",
    " 'Adoptee &amp; Genealogical Detective!',\n",
    " 'adult adoptee',\n",
    " 'multiracial // transracial adoptee | prioritizing adoptee voices',\n",
    " 'step adoptee',\n",
    " 'half-adopted, hap',\n",
    " 'Adoptee @ 106 Days &amp; Genealogical Detective!',\n",
    " 'Adoptee; Confused as Hell',\n",
    " 'Second-generation adoptee',\n",
    " 'TRA/ICA',\n",
    " 'Adoptee (domestic infant adoption)',\n",
    " 'Closed Adoption Infant Adoptee',\n",
    " 'UK Adoptee',\n",
    " 'Closed domestic (US) infant adoptee in reunion',\n",
    " 'adopted & hap',\n",
    " 'Adopted in the late 60‚Äôs',\n",
    " 'Transracial Adult Adoptee',\n",
    " 'Adoptee @ 106 Days & Genealogical Detective',\n",
    " 'Adoptee & AP',\n",
    " 'adoptee & parent',\n",
    " 'Domestic Infant Adoptee',\n",
    " 'üá∑üá∫',\n",
    " 'Closed domestic (US) adult adoptee in reunion',\n",
    " 'Black adult invisible adoptee',\n",
    " 'Chinese American Adoptee',\n",
    " 'Adoptee (US)',\n",
    " '60s scoop reunited',\n",
    " 'transracial adoptee',\n",
    " 'DIA in Reunion',\n",
    " 'Adoptee of Closed Adoption',\n",
    " 'Private Infant Adoptee - 25F',\n",
    " 'transracial closed adoptee',\n",
    " \"adoptee '87\",\n",
    " 'TRA / Chinese adoptee',\n",
    " 'Late discovery adoptee, 26 yrs. Met bio families.',\n",
    " 'Adult DIA Adoptee',\n",
    " 'late age adoptee',\n",
    " 'adoptee & birthparent',\n",
    " 'victim of domestic & state violence via transracial adoption',\n",
    " 'Adoptee, Birthmother, & Parent',\n",
    " 'Domestic Adoptee 1988',\n",
    " 'BIA adoptee',\n",
    " 'foster care (2007-2010) / adopted (2010)',\n",
    " 'adoptee open adoption',\n",
    " 'Adoptee and Psychologist',\n",
    " 'Who am I?',\n",
    " 'domestic infant(ish) adoptee',\n",
    " 'Adoptee of Failed Adoption',\n",
    " 'Adoptee and  Birthmother',\n",
    " 'adoptee + adoptive parent',\n",
    " 'Reunited Adoptee &amp; Adoptee Rights Activist',\n",
    " 'Adoptee, Adoptive Parent',\n",
    " 'adoptee 3.11.87',\n",
    " 'Adopted Faery',\n",
    " 'Child of two families'\n",
    " 'Adopted Kid',\n",
    " 'Adopted from Bangladesh',\n",
    " 'KAD wutup!',\n",
    " 'Adopted: birth.  Found bio siblings: age 20.',\n",
    " 'Adoptee, 29F',\n",
    " 'Closed adoption: birth. Found bio siblings: age 20.',\n",
    " 'Adoptee /  Adoptive Parent',\n",
    " 'Adoptee Found Birth Family',\n",
    " 'adoptee; foster parent',\n",
    " 'Adopted/Plans to adopt',\n",
    " 'Transracial Adoptee (KAD)',\n",
    " 'Birthmom+Adoptee',\n",
    " 'Adopted at Birth | Found Birthfamily',\n",
    " 'Foster Parent/Adoptee',\n",
    " 'Moderator, adoptee',\n",
    " 'Two moms, two dads, lucky reunited adoptee',\n",
    " 'adoptee / plans to adopt',\n",
    " 'Adopted 1993 | Reunited 2017',\n",
    " 'Adopted as a baby',\n",
    " 'Chinese adoptee',\n",
    " 'Adoptee (International)',\n",
    " 'Adoptee, Birthmother, Adoptive parent',\n",
    " 'Adoptee, Activist',\n",
    " 'Adopted from Russia',\n",
    " 'Adoptee - Found birth family',\n",
    " 'Punjabi-Canto interracial adoptee',\n",
    " 'Korean adoptee',\n",
    " 'Transracial Adoptee &amp; Birth Mother',\n",
    " 'Trans-Racial Adoptee | PAP | Anti-Natalist',\n",
    " 'Kazakh adoptee',\n",
    " 'late-discovery adoptee, ex-ward',\n",
    " 'LDA, ex-ward, Indigenous post-ICWA adoptee',\n",
    " 'Adult Adoptee/Found Bio Parents - Ohio 1986 Prive Adoption',\n",
    " 'Transnational Adoptee from Birth',\n",
    " \"author, the adoptee's guide to dna testing (book)\",\n",
    " 'LDA, ex-ward, Indigenous adoptee',\n",
    " 'Adoptee, Only Child',\n",
    " 'Closed Adoption Adoptee Reunited',\n",
    " 'late-discovery-adoptee, ex-foster-kid',\n",
    " 'Adoptee &amp; Adopter',\n",
    " 'Taiwanese Adoptee',\n",
    " 'r/Adoptee Moderator',\n",
    " 'Closed DIA',\n",
    " 'Adopted aged four',\n",
    " 'Adult adoptee',\n",
    " 'Adopted @11days - reunited @ 27y/o',\n",
    " 'TRA/IA/LDA/AP/FP',\n",
    " 'intrafamily adoptee, school aged adoptee',\n",
    " 'Adoptee/closed Birthmom/open',\n",
    " 'china adoptee',\n",
    " 'Childhood adoptee/Birthmother to now adult',\n",
    " 'First Nations Adoptee',\n",
    " 'Chinese Transracial Adoptee',\n",
    " 'second-generation adoptee',\n",
    " 'International adoptee',\n",
    " 'International Transracial Adoptee',\n",
    " 'From Russia with Love?',\n",
    " 'FFY/Adoptee',\n",
    " 'TransAdoptedKid',\n",
    " 'adoptee and 23 ‚úåÔ∏è',\n",
    " 'Adopted Person',\n",
    " 'adoptee/former foster kid',\n",
    " 'Pre-Adoptive Parent | Adopted',\n",
    " 'adoptee // 24',\n",
    " 'victim of domestic &amp; state violence via transracial adoption',\n",
    " 'cambodian adoptee',\n",
    " 'Adult Adoptee Found BioFamily',\n",
    " 'Birth adoptee reunited w/BM &amp; Half-Siblings',\n",
    " 'Adoptee, may consider adoption in the future',\n",
    " 'Adopted from China',\n",
    " 'Adopted at birth',\n",
    " 'Transracial Indigenous Adoptee',\n",
    " 'Adoptee from birth',\n",
    " 'Adopted Chinese',\n",
    " 'reunited adoptee',\n",
    " 'TRA/ICA/KAD',\n",
    " 'Adopted as an Infant',\n",
    " 'Transracial/international Adoptee',\n",
    " 'open adoptee from birth',\n",
    " 'Half-adopted',\n",
    " 'adoptee &amp; parent',\n",
    " 'Adopted at Birth',\n",
    " 'domestic adoptee at birth | found birthparents']\n",
    "\n",
    "# for regex will need to add \\\\ to escape the escape characters\n",
    "adoptee_labels = [re.escape(i) for i in adoptee_labels]\n",
    "# | to signify \"or\" for future regex\n",
    "adoptee_pattern = re.compile(r\"{}\".format(\"|\".join(adoptee_labels)), \n",
    "                             re.IGNORECASE)\n",
    "\n",
    "# Now do the same for non adoptee labels\n",
    "non_adoptee_labels = ['Former Foster Youth',\n",
    " 'Future AP',\n",
    " 'Bio Parent',\n",
    " 'Birthfather',\n",
    " 'Prospective Adoptive Parent',\n",
    " 'Birth Mother - Open Adoption',\n",
    " 'Mom through private domestic open transracial adoption',\n",
    " 'Reunited mother, former legal guardian, NPE',\n",
    " 'Reunited Mom, Foster Mom, L8 Dscvry Adoptee-paternal side',\n",
    " 'Reunited Birthparent.',\n",
    " 'Current Intl AP, Past Temp Foster Child',\n",
    " 'Birth Mom',\n",
    " 'AP, former FP, ASis',\n",
    " 'Birthmother.',\n",
    " 'birthmother',\n",
    " 'Birthmother 6/23/12',\n",
    " '14 adoptions in my family',\n",
    " 'Hopeful AP',\n",
    " 'Adoptive parent',\n",
    " 'birth parent',\n",
    " 'Adoptive Parent',\n",
    " 'Birthmother, 2002',\n",
    " 'Biological Father - searching',\n",
    " 'looking to adopt',\n",
    " 'adoptive father',\n",
    " 'Father of sibling group of 3',\n",
    " 'birth mom',\n",
    " 'Hopeful adopter',\n",
    " 'Adopting in Arkansas',\n",
    " 'Birthmom',\n",
    " 'Hoping to adopt',\n",
    " 'Looking into Adopting',\n",
    " 'caseyalexanderblog.wordpress.com',\n",
    " 'naturalmother_8-14-01',\n",
    " 'Someday-adopter',\n",
    " 'Potential Birthmother',\n",
    " 'waiting prospective AP',\n",
    " 'future AP',\n",
    " 'foster-to-adopt aunt/mom',\n",
    " 'Adoptive Dad',\n",
    " 'BP',\n",
    " 'I Fostered &amp; Then Adopted',\n",
    " 'Adoptive Parent/Orphanage Supervisor',\n",
    " 'Adoptive Parent (fostadopt)',\n",
    " 'hypervigilant.org',\n",
    " 'Possible AP',\n",
    " 'fost-adopt parent',\n",
    " 'pre-adoptive parent',\n",
    " 'Birth Parent',\n",
    " 'Birth Father &amp; /r/OpenAdoption Owner',\n",
    " 'RecentBM',\n",
    " 'Birthmother',\n",
    " 'Adoptive/Foster Mom',\n",
    " 'Adoptive Father',\n",
    " 'foster adopt',\n",
    " 'Adopted Family Member?',\n",
    " 'Prospective adoptive parent',\n",
    " 'Pre-fostering | prospective foster',\n",
    " 'First Mother',\n",
    " 'Bio of 2, Adoptive of 2',\n",
    " 'Sister adopted in x2, aunt adopted out x1',\n",
    " 'Hopeful APs',\n",
    " 'Researching foster/adoptive parenting',\n",
    " 'future FAD parent',\n",
    " 'Birthmother, Open Adoption',\n",
    " 'Adopting!',\n",
    " 'Adoptive Parent - Intercountry + Fostered',\n",
    " 'Post-Adoptions social worker', \n",
    " 'Adoptive Parent, Data Analyst',\n",
    " 'Bio sis',\n",
    " 'Adoptive/Foster Parent',\n",
    " 'ex-foster-kid',\n",
    " 'kinship adoptive parent / foster parent',\n",
    " 'Birthmother 2/13/2002',\n",
    " 'Transracial Adoptive Parent/Foster Parent',\n",
    " 'parent of adopted kids',\n",
    " 'Adoptive Mama',\n",
    " 'AP',\n",
    " 'Birthfather 10/21/1986',\n",
    " 'Interested, but no plans',\n",
    " 'Planning to Adopt in the Future',\n",
    " 'Future Foster/Adoptive Parent',\n",
    " 'Birth mother',\n",
    " 'Foster parent/Adoptive parent',\n",
    " 'Firstmom',\n",
    " 'Son, 12.. BirthMom',\n",
    " 'Father of 3, all adopted',\n",
    " 'kinship/foster parent',\n",
    " 'Momma',\n",
    " '23F- Future Adopter',\n",
    " 'Reunited Mom',\n",
    " 'Foster parent',\n",
    " 'Adoptive father',\n",
    " 'firstmother 2001',\n",
    " 'Adoptive mom of 3',\n",
    " 'Adoptive Dad of 3, soon 6',\n",
    " 'bio sibling',\n",
    " '5 failed matches, currently in #6 due winter 2016',\n",
    " 'Soon to be mom',\n",
    " 'Potential Adoptive Parent',\n",
    " 'foster/pre-adoptive parent',\n",
    " 'Intl Adoptive Parent',\n",
    " 'In Progress',\n",
    " 'Wanting to Adopt',\n",
    " 'Birth Father',\n",
    " '2 failed matches, still hoping',\n",
    " 'adoptive mom',\n",
    " '3 failed matches, still hoping',\n",
    " 'Someday-adopter, adoptive sister',\n",
    " 'Future adoptive parent',\n",
    " 'Matched with an expectant mom due in winter 2015',\n",
    " 'Adoptive Parents',\n",
    " 'Birthmother (Open Adoption)',\n",
    " 'AParent to teen',\n",
    " 'Son, 8.. BirthMom',\n",
    " 'Future Parent',\n",
    " 'considering adopting',\n",
    " 'Looking to adopt (Ontario)',\n",
    " 'Homestudied and waiting',\n",
    " 'Adopting thru Foster Care',\n",
    " 'sister of adoptee; future adoptive parent',\n",
    " 'Adoptive sister',\n",
    " 'Adoptive Dad of 3, soon 5',\n",
    " 'Adoption Researcher',\n",
    " 'was a foster parent',\n",
    " 'Parent',\n",
    " 'hoping to adopt',\n",
    " 'Father of 4 adopted sons',\n",
    " 'birthmom 2010, beautiful boy!',\n",
    " 'would like to adopt',\n",
    " 'Foster-to-Adopt',\n",
    " 'Birthmom 7/31/1992',\n",
    " 'potential adoptive father',\n",
    " '-25-groomer-wannabe adopter',\n",
    " 'Luckiest',\n",
    " 'Homestudied hopeful adopter',\n",
    " '3 adopted',\n",
    " 'sister of an adoptee',\n",
    " 'parent of several adopted kids',\n",
    " 'Birthmother, Daughter, Sister, Aunt, and Wife',\n",
    " 'adoptDad',\n",
    " '(b-mom, 1976)',\n",
    " 'Birthmom 3/15/98',\n",
    " 'Adoption Specialist',\n",
    " 'Fost-Adoptive parent of 3',\n",
    " 'HAP',\n",
    " 'Adoptive mom - open kinship',\n",
    " 'sister of adoptee; hopeful future AP',\n",
    " 'Birth Mom',\n",
    " 'haole, male, father to a daughter who was adopted, but not by me',\n",
    " 'may adopt in the future',\n",
    " 'LGBT adoptive parent &amp; daughter of adoptee',\n",
    " 'Adoptive Parent of Older Teen',\n",
    " 'prospective/pre-adoptive parent',\n",
    " 'Birthmom 2017',\n",
    " 'Birthmom 12/18/18',\n",
    " 'Furture adoptive mom, by choice.',\n",
    " 'Adoptive Parent x3',\n",
    " 'Future Father',\n",
    " 'Adoptive Parent &amp; Spouse to Adoptee',\n",
    " 'Adoptive Mama',\n",
    " 'prospective adoptive parent',\n",
    " 'Bio-Sis, Hopefully Future Adoptive Parent',\n",
    " 'Kinship AP',\n",
    " 'NY, Adoptive Parent, Permanency Specialist',\n",
    " 'Prospective AP',\n",
    " 'FFY - AP',\n",
    " 'Perspective adoptive parent',\n",
    " 'Adoptive Parent (International/Transracial)',\n",
    " 'Birth Mum.',\n",
    " 'Former foster kid. Almost-adopted more than once.',\n",
    " 'Adoptive Mother | Australia',\n",
    " 'Hoping to Adopt',\n",
    " 'Parent by Adoption',\n",
    " 'Researching PAP',\n",
    " 'One Adopted (Kinship), Seven Bio',\n",
    " 'Adoptive Parent (Kinship Via Husband)',\n",
    " 'foster mom',\n",
    " 'Younger Bio Sibling',\n",
    " 'Foster/Adoptive parent',\n",
    " 'AdoptiveParent',\n",
    " 'Adult Child of Adoptee',\n",
    " 'AP of teen',\n",
    " 'adoptive parent',\n",
    " 'Birth mom, 2017',\n",
    " 'Birth Mum of two - adopted by force.',\n",
    " 'buried under a pile of children',\n",
    " 'Adoptive mom',\n",
    " 'Daughter of 2 adoptees',\n",
    " 'Adoptive Mother',\n",
    " 'Reunited Birthparent.',\n",
    " 'Foster Parent',\n",
    " 'Adoptive Dad of 5 (2 sib grps from foster care)',\n",
    " 'Birthmother 12/13/2002',\n",
    " 'PAP',\n",
    " 'Prospective Parent',\n",
    " 'AP, former FP, ASis',\n",
    " 'biological parent',\n",
    " 'Birth Parent in StepParent Adoption',\n",
    " 'Open Adoption Birth Father &amp; /r/OpenAdoption Owner',\n",
    " 'Foster Parent, Child Welfare Public Health Professional',\n",
    " 'Have adopted-in siblings; searching for adopted-out sister',\n",
    " 'Pre-Placement Parent',\n",
    " '16|05.20.2020|Adoption',\n",
    " 'Prep-Adoptive',\n",
    " 'daughter of an adoptee',\n",
    " 'Adoptive Mom',\n",
    " 'reunited mom, lgl grdian, NPE',\n",
    " 'Birth Mother',\n",
    " 'NPE',\n",
    " 'Bio-Sib of an adoptee',\n",
    " 'Potential Foster Parent',\n",
    " 'Foster / Adoptive Parent',\n",
    " 'Foster Mom',\n",
    " 'mother was adopted',\n",
    " 'Stepmum to long lost adoptee / reunited',\n",
    " 'daughter of an adoptee. future adoptive parent.',\n",
    " 'Birthparent / Baby Girl due 12/28 :)',\n",
    " 'Prospective Adoptive Mother',\n",
    " 'Reunited Birthmom',\n",
    " 'Foster/Adoptive Parent',\n",
    " 'Hopeful Adoptive Parent',\n",
    " 'foster parent',\n",
    " 'Prospective Adoptive Parent',\n",
    " 'hopeful foster parent',\n",
    " '15 adoptions in my family',\n",
    " 'Reunited Birthparent.',\n",
    " 'Reunited Bio Mom',\n",
    " 'Adoptive Parent &amp; Adoptee‚Äôs spouse']\n",
    "\n",
    "non_adoptee_labels = [re.escape(i) for i in non_adoptee_labels]\n",
    "non_adoptee_pattern = re.compile(r\"{}\".format(\"|\".join(non_adoptee_labels)), \n",
    "                                 re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for opening reddit json\n",
    "def open_reddit_json(file_path):\n",
    "    \"\"\"\n",
    "    Takes a string of a json (of scrapped Reddit data) file and turns it into a dataframe\n",
    "    Inputs:\n",
    "        file_path (str): the file path of the JSON file\n",
    "    Returns pandas DataFrame of pertinent information\n",
    "    \"\"\"\n",
    "    file = open(file_path, \"r\")\n",
    "\n",
    "    users = []\n",
    "    user_flairs = []\n",
    "    titles = []\n",
    "    post_texts = []\n",
    "    post_dates = []\n",
    "    post_flairs = []\n",
    "    scores = []\n",
    "    n_comments_list = []\n",
    "    links = []\n",
    "\n",
    "    for line in file:\n",
    "        post = json.loads(line)\n",
    "\n",
    "        try:\n",
    "            users.append(post.get(\"author\", np.nan))\n",
    "            user_flairs.append(post.get(\"author_flair_text\", np.nan))\n",
    "            titles.append(post.get(\"title\", np.nan)) # will be nan for comments\n",
    "\n",
    "            # post text data is located in different places if comment vs post\n",
    "            post_text = post.get(\"selftext\")\n",
    "            if post_text:\n",
    "                post_texts.append(post_text)\n",
    "            else:\n",
    "                post_texts.append(post.get(\"body\", np.nan))\n",
    "\n",
    "            post_dates.append(post.get(\"created_utc\", np.nan))\n",
    "            post_flairs.append(post.get(\"link_flair_text\", np.nan))\n",
    "            scores.append(post.get(\"score\", np.nan))\n",
    "            n_comments_list.append(post.get(\"num_comments\", np.nan))\n",
    "            if post.get(\"permalink\"):\n",
    "                links.append(\"https://old.reddit.com\" + post.get(\"permalink\", np.nan))\n",
    "            else:\n",
    "                links.append(np.nan)       \n",
    "        except:\n",
    "            continue            \n",
    "\n",
    "    output = pd.DataFrame({\"user\": users,\n",
    "                           \"user_flair\": user_flairs,\n",
    "                           \"title\": titles,\n",
    "                           \"post_text\": post_texts,\n",
    "                           \"post_date\": post_dates,\n",
    "                           \"post_flair\": post_flairs,\n",
    "                           \"score\": scores,\n",
    "                           \"n_comments\": n_comments_list,\n",
    "                           \"link\": links})\n",
    "    return output\n",
    "\n",
    "\n",
    "def categorize_user(x):\n",
    "    \"\"\"\n",
    "    Given a user flair (str) categorize as either adoptee, non-adoptee, \n",
    "        or NEI (not enough info)\n",
    "    Input: X (str): a user flair\n",
    "    Returns: 1 if adoptee, 0 if non-adoptee, 2 if NEI    \n",
    "    \"\"\"\n",
    "    if adoptee_pattern.match(x):\n",
    "        return 1  # Adoptee\n",
    "    elif non_adoptee_pattern.match(x):\n",
    "        return 0  # Non-adoptee\n",
    "    else:\n",
    "        return 2  # NEI\n",
    "\n",
    "### HELPERS INSPIRED BY HW2 AND HW4 OF CONTENT ANALYSIS ###\n",
    "\n",
    "# NOT ACTUALLY USED\n",
    "# def word_tokenize(word_list):\n",
    "#     \"\"\"\n",
    "#     Tokenize a list of words or a single string\n",
    "#     Input: list of strings or a single string\n",
    "#     Returns a list of tokenized strings\n",
    "#     \"\"\"\n",
    "#     # Pass word list through language model\n",
    "#     if isinstance(word_list, str):\n",
    "#         word_list = [word_list]\n",
    "#     doc = nlp(\" \".join(word_list))    \n",
    "#     return [token.text for token in doc if not token.is_punct and token.text.strip()]\n",
    "\n",
    "\n",
    "# NOT ACTUALLY USED\n",
    "# def normalize_tokens(word_list):\n",
    "#     \"\"\"\n",
    "#     Tokenize and normalize a list of words\n",
    "#     Inputs:\n",
    "#         word_list: list of strings for words to be tokenized\n",
    "#     Returns a list of normalized strings\n",
    "#     \"\"\"\n",
    "#     # Convert word_list to a single string\n",
    "#     if isinstance(word_list, list):\n",
    "#         word_list = ' '.join(word_list)\n",
    "    \n",
    "#     # Tokenize the text and convert to lowercase\n",
    "#     doc = nlp(word_list.lower())\n",
    "\n",
    "#     # Extract normalized tokens\n",
    "#     normalized = [str(w.lemma_) for w in doc \n",
    "#                   if not w.is_stop and not w.is_punct \n",
    "#                   and not w.like_num and len(w.text.strip()) > 0]\n",
    "\n",
    "#     return normalized\n",
    "\n",
    "\n",
    "# USED, logic stems from above two functions\n",
    "def tokenize_and_normalize(word_list):\n",
    "    \"\"\"\n",
    "    combine the logic for the two functions above\n",
    "    \"\"\"\n",
    "    if isinstance(word_list, list):\n",
    "        word_list = ' '.join(word_list)\n",
    "    doc = nlp(word_list.lower())\n",
    "    tokenzied = [token.text for token in doc \n",
    "                 if not token.is_punct and token.text.strip()]\n",
    "    normalized = [str(w.lemma_) for w in doc \n",
    "                  if not w.is_stop and not w.is_punct \n",
    "                  and not w.like_num and len(w.text.strip()) > 0]\n",
    "\n",
    "    return tokenzied, normalized\n",
    "\n",
    "\n",
    "def tokenize_sents(word_list, model=nlp):\n",
    "    \"\"\"\n",
    "    Tokenize a list of words using a specified model.\n",
    "\n",
    "    Parameters:\n",
    "        word_list (list): A list of words to be tokenized into sentences.\n",
    "        model (Spacy model): the language model to be used for tokenization. \n",
    "            Defaults to nlp model.\n",
    "\n",
    "    Returns:\n",
    "        list: list of sentences extracted from the input text.\n",
    "    \"\"\"\n",
    "    doc = model(word_list)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def tag_sents_pos(sentences):\n",
    "    \"\"\"\n",
    "    function which replicates NLTK pos tagging on sentences.\n",
    "    \"\"\"\n",
    "    new_sents = []\n",
    "    for sentence in sentences:\n",
    "        new_sent = ' '.join(sentence)\n",
    "        new_sents.append(new_sent)\n",
    "    final_string = ' '.join(new_sents)\n",
    "    doc = nlp(final_string)\n",
    "\n",
    "    pos_sents = []\n",
    "    for sent in doc.sents:\n",
    "        pos_sent = []\n",
    "        for token in sent:\n",
    "            pos_sent.append((token.text, token.tag_))\n",
    "        pos_sents.append(pos_sent)\n",
    "\n",
    "    return pos_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My data comes from 3 sources in total and were combined in the above into one large data set. The first source was a reddit archival site which contained all posts from the subreddit r/Adoption until December of 2022. Furthermore I created a webscrawler and scrapper to gather more reddit posts from r/Adoption for more current posts and also to get data for r/Adopted (because the subreddit was smaller, it was not archived by the [site](https://the-eye.eu/redarcs/) I used). In total there are around 330k posts and comments in total. I keep track of a few variables: user, user flair (if any), post_title (if post, otherwise NaN), post_text, post_date, post_flair (if any), score (total of upvotes and downvotes), n_comments (if applicable), link, and subreddit. I have also created a few more columns for ease of analysis: is_comment, full_text (title + post text if applicable), cleaner_text, full_tokens, word_count, norm_tokens, token_sents, norm_sents, POS_sents, and is_adoptee. Please note that the all_df is actually created during the next section data cleaning and data wrangling but is presented here for ease of understanding the entire data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_df = pd.read_pickle(\"NEW_all_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at all of the variables within this data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that most of the columns are objects (usually strings or list of strings of some sort). Only a few are numeric (word_count, n_comments, and is_adoptee). We can see in the data cleaning section below that the is_adoptee column has values of 0, 1, and 2 (which was coded by hand)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning, Merging, Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for r/Adopted\n",
    "adopted_df = pd.read_csv(\"D:\\\\hw\\\\adopt-proj\\\\adopt_posts.csv\")\n",
    "# change to date time\n",
    "adopted_df['post_date'] = pd.to_datetime(adopted_df['post_date'])\n",
    "adopted_df['subreddit'] = \"r/Adopted\"\n",
    "\n",
    "# for r/Adoption\n",
    "adoption_1 = pd.read_csv(\"D:\\\\hw\\\\adopt-proj\\\\adoption_posts.csv\")\n",
    "\n",
    "adoption_2_posts = open_reddit_json(\"D:\\\\hw\\\\adopt-proj\\\\Adoption_submissions.json\")\n",
    "adoption_2_comms = open_reddit_json(\"D:\\\\hw\\\\adopt-proj\\\\Adoption_comments.json\")\n",
    "adoption_2_posts[\"is_comment\"] = False\n",
    "adoption_2_comms[\"is_comment\"] = True\n",
    "\n",
    "# fix post date data type\n",
    "adoption_1['post_date'] = pd.to_datetime(adoption_1['post_date'])\n",
    "adoption_2_comms['post_date'] = pd.to_datetime(adoption_2_comms['post_date'].astype(int), unit='s').dt.tz_localize(\"UTC\")\n",
    "adoption_2_posts['post_date'] = pd.to_datetime(adoption_2_posts['post_date'].astype(int), unit='s').dt.tz_localize(\"UTC\")\n",
    "\n",
    "# combine all sources of r/Adoption data\n",
    "adoption_df = pd.concat([adoption_1, adoption_2_posts, adoption_2_comms])\n",
    "adoption_df['subreddit'] = \"r/Adoption\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unedited dataframe is size: (332300, 11)\n"
     ]
    }
   ],
   "source": [
    "# combine both subreddits\n",
    "all_df = pd.concat([adopted_df, adoption_df])\n",
    "print(\"The unedited dataframe is size:\", all_df.shape)\n",
    "# drop na\n",
    "all_df.dropna(subset=['post_text'], inplace=True)\n",
    "# reset indices \n",
    "all_df.reset_index(drop=True, inplace=True)\n",
    "all_df[\"full_text\"] = (all_df.title.fillna(\"\") +  \". \" + \n",
    "                       all_df.post_text).replace(r\"^. \", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with nans or empty strings\n",
    "all_df = all_df[all_df.dropna(subset=['full_text']).full_text != \"\"]\n",
    "# for some reason there is this one weird character for score in one row\n",
    "all_df.replace(\"‚Ä¢\", np.nan, inplace=True)\n",
    "all_df[\"score\"] = all_df.score.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_posts = r\"^(?:\\(|\\[)?(?:deleted|removed)(?:\\)|\\])?[\\s-]*\"\n",
    "# filter out the useless posts\n",
    "all_df = all_df[~all_df['post_text'].str.contains(useless_posts, case=False, regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "all_df.drop_duplicates(subset=[\"user\",\"title\", \"subreddit\", \"post_text\", \"full_text\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because the info is stored in full_text, we don't need these columns anymore\n",
    "all_df.drop(columns=[\"title\", \"post_text\"], inplace=True)\n",
    "all_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's add labels and non labels\n",
    "all_df[\"is_adoptee\"] = all_df[\"user_flair\"].astype(str).apply(categorize_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have filtered out rows will not be using in the analysis, it is time to move onto further data cleaning and wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonenglish_syms = r\"\"\"[^a-zA-Z\\d \\.\\?\\,\\!\\-\\#\\:\\:\\\\\\\\\\/]\"\"\"\n",
    "straight_curly = r\"[\\‚Äô\\‚Äò\\‚Äú\\‚Äù]\"\n",
    "# filter created with help from ChatGPT\n",
    "url_pattern = r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop word stuff\n",
    "# set a stopwords set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# add more stop words\n",
    "custom_stopwords = set([\"could've\", \"would've\", \"r\", \"u/\", \"u\", \"/r\" \"r/\", \"t\", 've', 's', 'm', \n",
    "                        'll', 'd', 're', 'n', 'y', 'b', 'p', 'f', 'c', 'e', 'g', \n",
    "                        'h', 'j', 'k', 'l', 'o', 'q', 'v', 'w', 'x', 'z', 'a', 'i', \"gt\", \"amp\"])\n",
    "\n",
    "stop_words.update(custom_stopwords)\n",
    "remove_stops = r'\\b(?:{})\\b'.format('|'.join(stop_words))\n",
    "# update nlp stopwords vocab\n",
    "for stopword in stop_words:\n",
    "    nlp.vocab[stopword].is_stop = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[\"cleaner_text\"] = all_df.full_text.str.lower().replace(url_pattern, \" \",\n",
    "                                        regex=True).replace(nonenglish_syms, \" \", \n",
    "                                        regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the text (tokenize, normalize, POS tagging) for words and sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for progress_apply bars\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 302429/302429 [1:08:24<00:00, 73.69it/s] \n"
     ]
    }
   ],
   "source": [
    "all_df[\"tokens\"], all_df[\"norm_tokens\"] = zip(*all_df.cleaner_text.progress_apply(lambda x: tokenize_and_normalize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 302429/302429 [3:44:27<00:00, 22.46it/s]    \n"
     ]
    }
   ],
   "source": [
    "all_df[\"toke_sents\"], all_df[\"norm_sents\"] = zip(*all_df.cleaner_text.progress_apply(lambda x: zip(*[tokenize_and_normalize(s) for s in tokenize_sents(x)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 302429/302429 [1:05:45<00:00, 76.64it/s] \n"
     ]
    }
   ],
   "source": [
    "all_df['POS_sents'] = all_df.toke_sents.progress_apply(lambda x: tag_sents_pos(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add counts\n",
    "all_df[\"num_tokens\"] = all_df.tokens.apply(len)\n",
    "all_df[\"num_norm_tokens\"] = all_df.norm_tokens.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.to_pickle(\"UPDATED_all_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create models for adoptee and non adoptee\n",
    "adoptee_model = gensim.models.word2vec.Word2Vec(all_df[\n",
    "    all_df.is_adoptee == 1].norm_sents.explode().dropna().reset_index(drop=True))\n",
    "non_adoptee_model = gensim.models.word2vec.Word2Vec(all_df[\n",
    "    all_df.is_adoptee == 0].norm_sents.explode().dropna().reset_index(drop=True))\n",
    "adoptee_model.save(\"adoptee_word2vec.model\")\n",
    "non_adoptee_model.save(\"non_adoptee_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create full model for all data and data for each subreddit\n",
    "all_adopt_model = gensim.models.word2vec.Word2Vec(all_df[all_df.subreddit == \n",
    "        \"r/Adoption\"].norm_sents.explode().dropna().reset_index(drop=True))\n",
    "r_adopted_model = gensim.models.word2vec.Word2Vec(all_df[all_df.subreddit == \n",
    "        \"r/Adopted\"].norm_sents.explode().dropna().reset_index(drop=True))\n",
    "r_adoption_model = gensim.models.word2vec.Word2Vec(\n",
    "    all_df.norm_sents.explode().dropna().reset_index(drop=True))\n",
    "\n",
    "all_adopt_model.save(\"NEW_all_adopt_word2vec.model\")\n",
    "r_adopted_model.save(\"NEW_r_adopted_word2vec.model\")\n",
    "r_adoption_model.save(\"NEW_r_adoption_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models for the other data\n",
    "sub_X_train_sum = sub_X_train.norm_sents.explode().dropna().reset_index(drop=True)\n",
    "train_sub_model = gensim.models.word2vec.Word2Vec(sub_X_train_sum)\n",
    "train_sub_model.save(\"train_sub_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_X_test_sum = sub_X_test.norm_sents.explode().dropna().reset_index(drop=True)\n",
    "test_sub_model = gensim.models.word2vec.Word2Vec(sub_X_test_sum)\n",
    "test_sub_model.save(\"test_sub_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adp_X_train_sum = adp_X_train.norm_sents.explode().dropna().reset_index(drop=True)\n",
    "train_adp_model = gensim.models.word2vec.Word2Vec(adp_X_train_sum)\n",
    "train_adp_model.save(\"train_adp_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adp_X_train_sum = adp_X_test.norm_sents.explode().dropna().reset_index(drop=True)\n",
    "test_adp_model = gensim.models.word2vec.Word2Vec(adp_X_train_sum)\n",
    "test_adp_model.save(\"test_adp_word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_pickle(\"UPDATED_all_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_adopt_model = gensim.models.word2vec.Word2Vec.load(\"NEW_all_adopt_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "sub_X_train, sub_X_test, sub_y_train, sub_y_test = train_test_split(all_df.drop(\"subreddit\",\n",
    "                                                                                axis=1), \n",
    "                                                                    all_df.subreddit, \n",
    "                                                                    test_size=0.2, \n",
    "                                                                    random_state=42)\n",
    "\n",
    "adp_X_train, adp_X_test, adp_y_train, adp_y_test = train_test_split(all_df.drop(\"is_adoptee\",\n",
    "                                                                                axis=1), \n",
    "                                                                    all_df.is_adoptee, \n",
    "                                                                    test_size=0.2, \n",
    "                                                                    random_state=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
