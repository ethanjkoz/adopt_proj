{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import bs4\n",
    "import requests\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_reddit_json(file_path):\n",
    "    \"\"\"\n",
    "    Takes a string of a json (of scrapped Reddit data) file and turns it into a dataframe\n",
    "    Inputs:\n",
    "        file_path (str): the file path of the JSON file\n",
    "    Returns pandas DataFrame of pertinent information\n",
    "    \"\"\"\n",
    "    file = open(file_path, \"r\")\n",
    "\n",
    "    users = []\n",
    "    user_flairs = []\n",
    "    titles = []\n",
    "    post_texts = []\n",
    "    post_dates = []\n",
    "    post_flairs = []\n",
    "    scores = []\n",
    "    n_comments_list = []\n",
    "    #links = []\n",
    "\n",
    "    for line in file:\n",
    "        post = json.loads(line)\n",
    "\n",
    "        try:\n",
    "            users.append(post.get(\"author\", np.nan))\n",
    "            user_flairs.append(post.get(\"author_flair_text\", np.nan))\n",
    "            titles.append(post.get(\"title\", np.nan)) # will be nan for comments\n",
    "            \n",
    "            # post text data is located in different places if comment vs post\n",
    "            post_text = post.get(\"selftext\")\n",
    "            if post_text:\n",
    "                post_texts.append(post_text)\n",
    "            else:\n",
    "                post_texts.append(post.get(\"body\", np.nan))\n",
    "            post_dates.append(post.get(\"created_utc\", np.nan))\n",
    "            post_flairs.append(post.get(\"link_flair_text\", np.nan))\n",
    "            scores.append(post.get(\"score\", np.nan))\n",
    "            n_comments_list.append(post.get(\"num_comments\", np.nan))\n",
    "            #links.append(post.get(\"url\", np.nan))\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    output = pd.DataFrame({\"user\": users,\n",
    "                           \"user_flair\": user_flairs,\n",
    "                           \"title\": titles,\n",
    "                           \"post_text\": post_texts,\n",
    "                           \"post_date\": post_dates,\n",
    "                           \"post_flair\": post_flairs,\n",
    "                           \"score\": scores,\n",
    "                           \"n_comments\": n_comments_list #,\n",
    "                           # \"link\": links\n",
    "                           })\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WEB SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HELPERS ###\n",
    "# OVERALL TASK IS TO SCRAPE REDDIT FOR POSTS AND COMMENTS\n",
    "\n",
    "## SHOULD CHANGE THE ATTRS FUNCs TO ADD SUBREDDIT##\n",
    "\n",
    "\n",
    "def get_next_page(soup):\n",
    "    \"\"\"\n",
    "    Find the next page of a subreddit\n",
    "    Input: soup (bs4 soup): an html soup of a subreddit page\n",
    "    Returns str or None if it finds a next page\n",
    "    \"\"\"\n",
    "    next_page = soup.find(\"span\", class_=\"next-button\")\n",
    "    if next_page:\n",
    "        return next_page.find(\"a\").get(\"href\")\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def reddit_crawler(domain_URL, r_headers, wait_times, max_pages, csv_filename):\n",
    "    \"\"\"\n",
    "    Crawls a reddit by page, obtaining a link for each page on subreddit\n",
    "    returns a tuple of the visited_pages, visited_pages_soup, urls_to_visit\n",
    "    \n",
    "    OUTPUTS\n",
    "    visited_pages (list) of page urls\n",
    "    visited_pages_soup (list) of page soups (same len as visited_pages)\n",
    "    urls_to_visit (list) of all comment urls on every page \n",
    "    \n",
    "    also creates a .csv file with the link and soup for the webpage\n",
    "    \"\"\"\n",
    "    visited_pages = []\n",
    "    urls_to_visit = []\n",
    "    visited_pages_soup = []\n",
    "    num_pages_visited = 0\n",
    "    curr_url = domain_URL # initialize curr_url as domain_URL\n",
    "    min_time, max_time = wait_times\n",
    "    \n",
    "    # create a csv file\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        # Create a CSV writer object\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "\n",
    "        # Check if the file is empty (write header if needed)\n",
    "        if csvfile.tell() == 0:\n",
    "            csv_writer.writerow([\"link\", \"soup\"])\n",
    "        \n",
    "        # while the number of pages is less than the max\n",
    "        while num_pages_visited < max_pages:\n",
    "            # add current page to visited pages\n",
    "            num_pages_visited += 1\n",
    "\n",
    "            print(\"Pages visited:\", num_pages_visited)\n",
    "\n",
    "            # get request for webpage\n",
    "            request = requests.get(curr_url, headers=r_headers)\n",
    "\n",
    "            # give the server a break\n",
    "            time.sleep(random.uniform(min_time, max_time))\n",
    "\n",
    "            # if request is valid\n",
    "            if request.status_code == 200:\n",
    "                soup_page = bs4.BeautifulSoup(request.text, \"html.parser\")\n",
    "                \n",
    "                # update the CSV with this new url and page\n",
    "                csv_writer.writerow([curr_url, soup_page])\n",
    "\n",
    "                # updated visited pages soup list\n",
    "                visited_pages_soup.append(soup_page)\n",
    "                visited_pages.append(curr_url)\n",
    "\n",
    "                # find all the comment links\n",
    "                links_html = soup_page.find_all(\"a\", class_ = re.compile(r\"bylink comments\"))\n",
    "                links = [link.get(\"href\") for link in links_html]\n",
    "                # add links to visited_urls\n",
    "                urls_to_visit += links\n",
    "\n",
    "                next_page = get_next_page(soup_page)\n",
    "                if next_page:\n",
    "                    curr_url = next_page\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # if unable to get current URL request, return visited URLs\n",
    "            else:\n",
    "                print(f\"Response {request.status_code}. Couldn't get URL: {curr_url}.\")\n",
    "                break\n",
    "        \n",
    "    return visited_pages, visited_pages_soup, urls_to_visit\n",
    "\n",
    "\n",
    "def find_post_attrs(post):\n",
    "    \"From post, find important post attributes for a Reddit post, a list\"\n",
    "    if not post:\n",
    "        return [None]*9\n",
    "    # find the user\n",
    "    try:\n",
    "        user = post.find(\"div\").get(\"data-author\")\n",
    "    except:\n",
    "        user = None\n",
    "    try:\n",
    "        user_flair = post.find(\"p\", class_=\"tagline\").find(\"span\", class_=re.compile(\"flair\")).text\n",
    "    except:\n",
    "        user_flair = None\n",
    "\n",
    "    try:\n",
    "        title = post.find(\"a\", class_=re.compile(\"title\")).text\n",
    "    except:\n",
    "        title = None\n",
    "    \n",
    "    try:\n",
    "        post_text = post.find(\"div\", class_=\"md\").text\n",
    "    except:\n",
    "        post_text = None\n",
    "    \n",
    "    try:\n",
    "        post_date = post.find(\"div\").find(\"time\").get(\"datetime\")\n",
    "    except:\n",
    "        post_date = None\n",
    "\n",
    "    try:\n",
    "        post_flair = post.find(\"p\", class_=re.compile(\"title\")).find(\"span\", class_=re.compile(\"flair\")).text\n",
    "    except:\n",
    "        post_flair = None\n",
    "\n",
    "    try:\n",
    "        score = post.find(\"div\", class_=\"score unvoted\").text\n",
    "    except:\n",
    "        score = None\n",
    "\n",
    "    try: \n",
    "        n_comments = post.find(\"div\").get(\"data-comments-count\")\n",
    "\n",
    "    except:\n",
    "        n_comments = None\n",
    "\n",
    "    try:\n",
    "        link = post.find(\"a\", class_=re.compile(r\"comments\")).get(\"href\")\n",
    "    except:\n",
    "        link = None\n",
    "    post_attrs = [user, user_flair, title, post_text, \n",
    "                  post_date, post_flair, score, n_comments, link]\n",
    "\n",
    "    return post_attrs\n",
    "\n",
    "\n",
    "def find_com_attrs(comment, link):\n",
    "    if not comment:\n",
    "        return [None]*9\n",
    "    try:\n",
    "        user = comment.find(\"a\", class_=re.compile(\"author\")).text\n",
    "    except:\n",
    "        user = None\n",
    "    try:\n",
    "        user_flair = comment.find(\"span\", class_= re.compile(r\"flair\")).get(\"title\")\n",
    "    except:\n",
    "        user_flair = None\n",
    "    try:\n",
    "        com_text = comment.find(\"div\", class_=\"md\").text\n",
    "    except:\n",
    "        com_text = None\n",
    "    try:\n",
    "        com_date = comment.find(\"time\").get(\"datetime\")\n",
    "    except:\n",
    "        com_date = None\n",
    "\n",
    "    try:\n",
    "        com_score = int(comment.find(\"span\", class_=\"score unvoted\").get(\"title\", 0))\n",
    "    except:    \n",
    "        com_score = None\n",
    "\n",
    "    com_attrs = [user, user_flair, None, com_text, \n",
    "                 com_date, None, com_score, None, link]\n",
    "    \n",
    "    return com_attrs\n",
    "\n",
    "\n",
    "def reddit_scraper(page_soups, r_headers, wait_times, csv_filename, max_com):\n",
    "    min_time, max_time = wait_times\n",
    "    '''\n",
    "    Scrape all the post and comment text for a given subreddit page, return\n",
    "    all post soups\n",
    "    '''\n",
    "\n",
    "    all_post_soups = []\n",
    "    \n",
    "    # create CSV file\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        # initialize column names if blank\n",
    "        if csvfile.tell() == 0:\n",
    "            csv_writer.writerow([\"user\", \"user_flair\", \"title\", \"post_text\", \n",
    "                                 \"post_date\", \"post_flair\", \"score\", \n",
    "                                 \"n_comments\", \"link\", \"is_comment\"])\n",
    "        \n",
    "        # iterate over all webpage soups\n",
    "        for page_soup in page_soups:\n",
    "            site_table = page_soup.find(\"div\", id=\"siteTable\")\n",
    "            posts = site_table.find_all(\"div\", class_=re.compile(r\"thing\"))\n",
    "            posts_links = [p.find(\"a\", class_= re.compile(\"bylink\")).get(\"href\") for p in posts]\n",
    "\n",
    "            for post_link in posts_links:\n",
    "                time.sleep(random.uniform(min_time, max_time))\n",
    "                \n",
    "                request = requests.get(post_link, headers=r_headers)\n",
    "\n",
    "                if request.status_code == 200:\n",
    "\n",
    "                    # find the post\n",
    "                    post_soup = bs4.BeautifulSoup(request.text, \"html.parser\")\n",
    "                    post = post_soup.find(\"div\", class_=re.compile(r\"sitetable\"))\n",
    "                    post_attrs = find_post_attrs(post)\n",
    "                    all_post_soups.append(post_soup)\n",
    "                    csv_writer.writerow(post_attrs + [False])\n",
    "\n",
    "                    # print(\"\\t### MOVING ON TO COMMENT SECTION ###\")\n",
    "                    \n",
    "                    # find the comment section\n",
    "                    comment_section = post_soup.find(\"div\", class_= \"sitetable nestedlisting\")\n",
    "                    comments = comment_section.find_all(\"div\", class_=\"entry unvoted\")\n",
    "\n",
    "                    for comment in comments[:max_com]:\n",
    "                        com_attrs = find_com_attrs(comment, post_link)\n",
    "                        \n",
    "                        csv_writer.writerow(com_attrs + [True])\n",
    "                    \n",
    "                    # print(f\"\\t\\t## STOPPED ##\\n\\t\\t## MOVING TO NEXT POST ##\")\n",
    "\n",
    "                else:\n",
    "                    # if we fail to get request, find the link that did it\n",
    "                    print(f\"!!!! FAILURE !!!!\\nResponse {request.status_code}. Couldn't get URL: {post_link}.\")\n",
    "                    \n",
    "    \n",
    "    return all_post_soups\n",
    "\n",
    "\n",
    "def get_data(domain_URL, r_headers, wait_times, max_pages, max_com, pages_csv, posts_csv):\n",
    "    \"\"\"\n",
    "    Given a subreddit, find all posts and comments\n",
    "    for debugging purposes, returns soups for all posts\n",
    "\n",
    "    INPUTS\n",
    "        max_pages limits the number of pages to crawl\n",
    "        max_com is for limiting the number of comments per post to scrape\n",
    "\n",
    "    ALSO \n",
    "    gets creates two CSV files one for tracking page soups, the other for the actual data\n",
    "\n",
    "    \"\"\"\n",
    "    # get soup for EACH webpage in a subreddit\n",
    "    _, soups, _ = reddit_crawler(domain_URL, r_headers, wait_times, max_pages, pages_csv)\n",
    "\n",
    "    print(\"##### DONE CRAWLING #####\\nMOVING ON TO SCRAPING FOR POSTS AND COMMENTS\")\n",
    "\n",
    "    post_soups = reddit_scraper(soups, r_headers, wait_times, posts_csv, max_com)\n",
    "    \n",
    "    print(\"##### DONE SCRAPING #####\")\n",
    "    \n",
    "    return soups, post_soups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once\n",
    "soups = get_data(\"https://old.reddit.com/r/Adopted/\", {\"User-Agent\": \"Ethan K.\"}, (1, 2), 1000, 150, 'adopt_pages.csv', \"adopt_posts.csv\")\n",
    "pd.Series(soups).to_csv(\"all_adopted_posts_soups.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DATA CLEANING #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dfs from csvs\n",
    "adopted = pd.read_csv(\"D:\\\\hw\\\\adopt-proj\\\\adopt_posts.csv\")\n",
    "adoption_1 = pd.read_csv(\"D:\\\\hw\\\\adopt-proj\\\\adoption_posts.csv\")\n",
    "\n",
    "# # drop link column\n",
    "# adopted.drop(columns=[\"link\"], inplace=True)\n",
    "# adoption_1.drop(columns=[\"link\"], inplace=True)\n",
    "\n",
    "# change this to datetime instead of str\n",
    "adopted['post_date'] = pd.to_datetime(adopted['post_date'])\n",
    "adoption_1['post_date'] = pd.to_datetime(adoption_1['post_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split r/Adopted into comments and posts\n",
    "adopted_comms = adopted.loc[adopted[\"is_comment\"] == True]\n",
    "# adopted_comms = adopted_comms.drop(columns=[\"is_comment\"])\n",
    "\n",
    "adopted_posts = adopted.loc[adopted[\"is_comment\"] == False]\n",
    "# adopted_posts = adopted_posts.drop(columns=[\"is_comment\"])\n",
    "\n",
    "\n",
    "# split r/Adoption into comments and posts\n",
    "adoption_1_comms = adoption_1.loc[adoption_1[\"is_comment\"] == True]\n",
    "# adoption_1_comms = adoption_1_comms.drop(columns=[\"is_comment\"])\n",
    "\n",
    "adoption_1_posts = adoption_1.loc[adoption_1[\"is_comment\"] == False]\n",
    "# adoption_1_posts = adoption_1_posts.drop(columns=[\"is_comment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some supplementary data from archived source\n",
    "adoption_2_posts = open_reddit_json(\"D:\\\\hw\\\\adopt-proj\\\\Adoption_submissions.json\")\n",
    "adoption_2_comms = open_reddit_json(\"D:\\\\hw\\\\adopt-proj\\\\Adoption_comments.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Trailing data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m temp \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mhw\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124madopt-proj\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mAdoption_comments.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m temp\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\Ethan\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:815\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\Ethan\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1025\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_lines(data_lines))\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1025\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1027\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mconvert_dtypes(\n\u001b[0;32m   1028\u001b[0m         infer_objects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype_backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_backend\n\u001b[0;32m   1029\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Ethan\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1051\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m   1049\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1051\u001b[0m     obj \u001b[38;5;241m=\u001b[39m FrameParser(json, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mparse()\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Ethan\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1187\u001b[0m, in \u001b[0;36mParser.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 1187\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse()\n\u001b[0;32m   1189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ethan\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1400\u001b[0m, in \u001b[0;36mFrameParser._parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1396\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[0;32m   1398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1399\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[1;32m-> 1400\u001b[0m         ujson_loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1401\u001b[0m     )\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1403\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1404\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[0;32m   1405\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m ujson_loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1406\u001b[0m     }\n",
      "\u001b[1;31mValueError\u001b[0m: Trailing data"
     ]
    }
   ],
   "source": [
    "temp = pd.read_json(\"D:\\\\hw\\\\adopt-proj\\\\Adoption_comments.json\")\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the values in post_date to datetime objects\n",
    "\n",
    "adoption_2_comms['post_date'] = pd.to_datetime(adoption_2_comms['post_date'].astype(int), unit='s').dt.tz_localize(\"UTC\")\n",
    "adoption_2_posts['post_date'] = pd.to_datetime(adoption_2_posts['post_date'].astype(int), unit='s').dt.tz_localize(\"UTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "adoption_2_comms[\"is_comment\"] = True\n",
    "\n",
    "# concat data sets\n",
    "adoption_comms = pd.concat([adoption_1_comms,adoption_2_comms])\n",
    "adoption_posts = pd.concat([adoption_1_posts,adoption_2_posts])\n",
    "\n",
    "# change this one weird quirk\n",
    "adoption_posts.loc[adoption_posts[\"score\"] == \"‚Ä¢\",\"score\"] = np.nan\n",
    "\n",
    "\n",
    "# remove posts with no post text, also remove duplicates\n",
    "adoption_posts = adoption_posts.dropna(subset=[\"user\", \"post_text\"]).drop_duplicates()\n",
    "adoption_comms = adoption_comms.dropna(subset=[\"user\", \"post_text\"]).drop_duplicates()\n",
    "# also do the same for r/Adopted, but this should do nothing\n",
    "adopted_posts = adopted_posts.dropna(subset=[\"user\", \"post_text\"]).drop_duplicates()\n",
    "adopted_comms = adopted_comms.dropna(subset=[\"user\", \"post_text\"]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find posts where they have been removed or deleted\n",
    "pattern = re.compile(r\"^\\[*\\(*(removed|deleted)\")\n",
    "# exclude such posts from the datasets\n",
    "adoption_posts = adoption_posts.loc[~adoption_posts[\"post_text\"].astype(str).apply(lambda x: True if pattern.match(x) else False)]\n",
    "adoption_comms = adoption_comms.loc[~adoption_comms[\"post_text\"].astype(str).apply(lambda x: True if pattern.match(x) else False)]\n",
    "adopted_posts = adopted_posts.loc[~adopted_posts[\"post_text\"].astype(str).apply(lambda x: True if pattern.match(x) else False)]\n",
    "adopted_comms = adopted_comms.loc[~adopted_comms[\"post_text\"].astype(str).apply(lambda x: True if pattern.match(x) else False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove posts that are just empty strings\n",
    "adoption_posts = adoption_posts[adoption_posts[\"post_text\"] != \"\"].reset_index(drop=True)\n",
    "adoption_comms = adoption_comms[adoption_comms[\"post_text\"] != \"\"].reset_index(drop=True)\n",
    "# should not really change much for r/Adopted data I scraped myself\n",
    "adopted_posts = adopted_posts[adopted_posts[\"post_text\"] != \"\"].reset_index(drop=True)\n",
    "adopted_comms = adopted_comms[adopted_comms[\"post_text\"] != \"\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEE BELOW\n",
    "\n",
    "# # Find adoptees by user flair tags\n",
    "# pattern = re.compile(r\"((?<!of an )(adoptee(?!s)\\b|adopted|\\bTRA\\b|\\bKAD\\b))|(?<!of a )(adoptee(?!s)\\b|adopted(?!kid)|adopted(?!child)|\\bTRA\\b|\\bKAD\\b)\", re.IGNORECASE)\n",
    "# adoption_posts[\"is_adoptee\"] = adoption_posts[\"user_flair\"].astype(str).apply(lambda x: True if pattern.match(x) else False)\n",
    "# adoption_comms[\"is_adoptee\"] = adoption_comms[\"user_flair\"].astype(str).apply(lambda x: True if pattern.match(x) else False)\n",
    "# adopted_posts[\"is_adoptee\"] = adopted_posts[\"user_flair\"].astype(str).apply(lambda x: True if pattern.match(x) else False)\n",
    "# adopted_comms[\"is_adoptee\"] = adopted_comms[\"user_flair\"].astype(str).apply(lambda x: True if pattern.match(x) else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save as csv\n",
    "# adoption_posts.to_csv(\"adoption_posts_df.csv\",index=False)\n",
    "# adoption_comms.to_csv(\"adoption_comms_df.csv\",index=False)\n",
    "# adopted_posts.to_csv(\"adopted_posts_df.csv\",index=False)\n",
    "# adopted_comms.to_csv(\"adopted_comms_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy is a good module to use for textual analyses\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# we will require this function later\n",
    "# function from HW2 of Content Analysis\n",
    "\n",
    "\n",
    "def word_tokenize(word_list):\n",
    "    tokenized = []\n",
    "    # pass word list through language model.\n",
    "    doc = nlp(word_list)\n",
    "    for token in doc:\n",
    "        if not token.is_punct and len(token.text.strip()) > 0:\n",
    "            tokenized.append(token.text)\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def normalizeTokens(word_list, extra_stop=[]):\n",
    "    #We can use a generator here as we just need to iterate over it\n",
    "    normalized = []\n",
    "    if type(word_list) == list and len(word_list) == 1:\n",
    "        word_list = word_list[0]\n",
    "\n",
    "    if type(word_list) == list:\n",
    "        word_list = ' '.join([str(elem) for elem in word_list])\n",
    "\n",
    "    doc = nlp(word_list.lower())\n",
    "\n",
    "    # add the property of stop word to words considered as stop words\n",
    "    if len(extra_stop) > 0:\n",
    "        for stopword in extra_stop:\n",
    "            lexeme = nlp.vocab[stopword]\n",
    "            lexeme.is_stop = True\n",
    "\n",
    "    for w in doc:\n",
    "        # if it's not a stop word or punctuation mark, add it to our article\n",
    "        if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and len(w.text.strip()) > 0:\n",
    "            # we add the lematized version of the word\n",
    "            normalized.append(str(w.lemma_))\n",
    "\n",
    "    return normalized\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "adp_df = adopted_posts\n",
    "adc_df = adopted_comms\n",
    "anp_df = adoption_posts\n",
    "anc_df = adoption_comms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([adp_df, anp_df, adc_df, anc_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>user_flair</th>\n",
       "      <th>title</th>\n",
       "      <th>post_text</th>\n",
       "      <th>post_date</th>\n",
       "      <th>post_flair</th>\n",
       "      <th>score</th>\n",
       "      <th>n_comments</th>\n",
       "      <th>link</th>\n",
       "      <th>is_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lenaballerina</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Does search squad help people internationally?...</td>\n",
       "      <td>2015-07-12 13:20:07+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://old.reddit.com/r/Adoption/comments/3cz...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>surf_wax</td>\n",
       "      <td>Adoptee</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm not exactly sure how they get their inform...</td>\n",
       "      <td>2015-07-12 16:01:13+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://old.reddit.com/r/Adoption/comments/3cz...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ST_MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm a 26 year old adoptee. I was put up for ad...</td>\n",
       "      <td>2015-09-04 16:16:40+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://old.reddit.com/r/Adoption/comments/3cz...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bettysonia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This a nice work\\n</td>\n",
       "      <td>2022-01-31 12:33:40+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://old.reddit.com/r/Adoption/comments/3cz...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mswihart</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>If I could submit a suggested add to the 23and...</td>\n",
       "      <td>2023-05-23 16:28:33+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://old.reddit.com/r/Adoption/comments/3cz...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273106</th>\n",
       "      <td>theferal1</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Please don‚Äôt look at adoption as a ‚Äúwaste‚Äù if ...</td>\n",
       "      <td>2022-12-31 22:21:45+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273107</th>\n",
       "      <td>wigglebuttbiscuits</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I invited her to PM me because we‚Äôve messaged ...</td>\n",
       "      <td>2022-12-31 22:34:12+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273108</th>\n",
       "      <td>theferal1</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Am I wrong are you not a hopeful adoptive pare...</td>\n",
       "      <td>2022-12-31 22:39:08+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273109</th>\n",
       "      <td>Francl27</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remember that open adoptions are not legally e...</td>\n",
       "      <td>2022-12-31 22:56:49+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273110</th>\n",
       "      <td>Teku98</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thank you so much for your reply, honestly jus...</td>\n",
       "      <td>2022-12-31 23:02:20+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>273111 rows √ó 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      user user_flair title  \\\n",
       "0            Lenaballerina        NaN   NaN   \n",
       "1                 surf_wax    Adoptee   NaN   \n",
       "2                    ST_MA        NaN   NaN   \n",
       "3               bettysonia        NaN   NaN   \n",
       "4                 mswihart        NaN   NaN   \n",
       "...                    ...        ...   ...   \n",
       "273106           theferal1       None   NaN   \n",
       "273107  wigglebuttbiscuits       None   NaN   \n",
       "273108           theferal1       None   NaN   \n",
       "273109            Francl27       None   NaN   \n",
       "273110              Teku98       None   NaN   \n",
       "\n",
       "                                                post_text  \\\n",
       "0       Does search squad help people internationally?...   \n",
       "1       I'm not exactly sure how they get their inform...   \n",
       "2       I'm a 26 year old adoptee. I was put up for ad...   \n",
       "3                                      This a nice work\\n   \n",
       "4       If I could submit a suggested add to the 23and...   \n",
       "...                                                   ...   \n",
       "273106  Please don‚Äôt look at adoption as a ‚Äúwaste‚Äù if ...   \n",
       "273107  I invited her to PM me because we‚Äôve messaged ...   \n",
       "273108  Am I wrong are you not a hopeful adoptive pare...   \n",
       "273109  Remember that open adoptions are not legally e...   \n",
       "273110  Thank you so much for your reply, honestly jus...   \n",
       "\n",
       "                       post_date post_flair score  n_comments  \\\n",
       "0      2015-07-12 13:20:07+00:00        NaN     7         NaN   \n",
       "1      2015-07-12 16:01:13+00:00        NaN     2         NaN   \n",
       "2      2015-09-04 16:16:40+00:00        NaN     7         NaN   \n",
       "3      2022-01-31 12:33:40+00:00        NaN     2         NaN   \n",
       "4      2023-05-23 16:28:33+00:00        NaN     5         NaN   \n",
       "...                          ...        ...   ...         ...   \n",
       "273106 2022-12-31 22:21:45+00:00        NaN    18         NaN   \n",
       "273107 2022-12-31 22:34:12+00:00        NaN     4         NaN   \n",
       "273108 2022-12-31 22:39:08+00:00        NaN     0         NaN   \n",
       "273109 2022-12-31 22:56:49+00:00        NaN    17         NaN   \n",
       "273110 2022-12-31 23:02:20+00:00        NaN     2         NaN   \n",
       "\n",
       "                                                     link  is_comment  \n",
       "0       https://old.reddit.com/r/Adoption/comments/3cz...        True  \n",
       "1       https://old.reddit.com/r/Adoption/comments/3cz...        True  \n",
       "2       https://old.reddit.com/r/Adoption/comments/3cz...        True  \n",
       "3       https://old.reddit.com/r/Adoption/comments/3cz...        True  \n",
       "4       https://old.reddit.com/r/Adoption/comments/3cz...        True  \n",
       "...                                                   ...         ...  \n",
       "273106                                                NaN        True  \n",
       "273107                                                NaN        True  \n",
       "273108                                                NaN        True  \n",
       "273109                                                NaN        True  \n",
       "273110                                                NaN        True  \n",
       "\n",
       "[273111 rows x 10 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### DON'T RUN AGAIN ####\n",
    "# # for this task, merge them\n",
    "\n",
    "# # get the dataframes\n",
    "\n",
    "\n",
    "# all_df = pd.concat([adp_df, adc_df, anp_df, anc_df]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# # takes around 66 minutes to run\n",
    "# all_df[\"tokenized_text\"] = all_df.loc[:,'post_text'].apply(lambda x: word_tokenize(x))\n",
    "# all_df['word_counts'] = all_df.loc[:,'tokenized_text'].apply(lambda x: len(x))\n",
    "# # takes around 61 minutes to run\n",
    "# all_df['normalized_tokens'] = all_df['tokenized_text'].apply(lambda x: normalizeTokens(x))\n",
    "# all_df['normalized_tokens_count'] = all_df['normalized_tokens'].apply(lambda x: len(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New stuff 2/12: fixing issues with labels being wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "adopted_i = len(adp_df) + len(adc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_pickle(\"all_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labels for subreddit\n",
    "all_df['sub'] = 'r/Adoption'  # Initialize the column with 'r/Adoption'\n",
    "all_df.loc[:adopted_i, 'sub'] = 'r/Adopted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "470"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniques = all_df.user_flair.unique()\n",
    "# for i in uniques:\n",
    "#     print(i)\n",
    "len(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('D:\\\\hw\\\\adopt-proj\\\\labels.txt', 'r') as file:\n",
    "#     # Read the lines of the file and store them in a list\n",
    "#     labels = file.readlines()\n",
    "\n",
    "# # Strip newline characters from each line and create a list\n",
    "# labels = [line.strip() for line in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern = re.compile(r\"((?<!of an )(adoptee(?!s)\\b|adopted|\\bTRA\\b|\\bKAD\\b))|(?<!of a )(adoptee(?!s)\\b|adopted(?!kid)|adopted(?!child)|\\bTRA\\b|\\bKAD\\b)\", re.IGNORECASE)\n",
    "# all_df[\"is_adoptee\"] = all_df[\"user_flair\"].astype(str).apply(lambda x: True if pattern.match(x) else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "adoptee_labels = ['Adoptee (UK)',\n",
    " 'Domestic Infant Adoptee',\n",
    " 'Transracial Adoptee',\n",
    " 'Adoptee',\n",
    " 'International Adoptee',\n",
    " 'Baby Scoop Era Adoptee',\n",
    " 'Adult Adoptee (DIA)',\n",
    " 'Mentally ill adopted teen',\n",
    " 'adopted at birth',\n",
    " 'Failed Adoptee',\n",
    " 'Reunited Adoptee',\n",
    " 'Adoptee ‚ù§Ô∏è',\n",
    " 'Adopted at 2 from Ukraine to the USA',\n",
    " 'Teen Adoptee, open adoption',\n",
    " 'Adoptee and Birth Parent',\n",
    " 'in adoption limbo ...',\n",
    " 'adopted family divorcee, adopted by birth mom',\n",
    " 'adoptee in reunion',\n",
    " 'adoptee',\n",
    " 'Korean-American  Adoptee',\n",
    " 'Adoptee with 6 parents',\n",
    " 'Happily reunited adoptee',\n",
    " '32/M/adoptee/in-reunion',\n",
    " 'Adopted @ 6yo',\n",
    " '(Adoptee, 1973)',\n",
    " 'Adoptee, looking',\n",
    " 'Adoptee/Step Dad',\n",
    " 'Adoptee and Birthmother',\n",
    " 'Adoptee recently reunited',\n",
    " 'Adopted',\n",
    " 'Adoptee Moderator',\n",
    " 'Adopted, trad/closed, Ohio',\n",
    " 'Adoptee, Foster Mama',\n",
    " 'Adult Adoptee',\n",
    " 'Closed At-Birth Adoptee',\n",
    " 'Foster Adoptee',\n",
    " 'Reunited adoptee',\n",
    " 'adoptee, closed adoption, seeking reunification',\n",
    " 'Chinese Adoptee',\n",
    " 'late-discovery-adoptee',\n",
    " 'Korean Adoptee, Married, CF',\n",
    " 'Adopted at birth',\n",
    " 'Russian - Kiwi Adoptee',\n",
    " 'Not Quite Adopted',\n",
    " 'adopted',\n",
    " 'INFP: The Dreamer',\n",
    " 'Transracial adoptee',\n",
    " 'Closed Adoption Adoptee',\n",
    " 'Adoptee Recently Reuinited',\n",
    " 'transracial &amp; transnational adoptee',\n",
    " 'Transracial US Domestic Adult Adoptee',\n",
    " 'Korean Adoptee',\n",
    " 'LDA, FFY, Indigenous adoptee',\n",
    " 'Adoptee',\n",
    " 'Chinese Adoptee in Canada (23F)',\n",
    " '1970 Closed Adoption Adoptee',\n",
    " 'adopted from China at 12mo',\n",
    " 'Open Adoption Adoptee',\n",
    " 'LDA, FFY, Indigenous adoptee',\n",
    " 'Russian Adoptee',\n",
    " 'International adoptive mom of two (Vietnam)',\n",
    " 'Russian adoptee',\n",
    " 'neo city üíö',\n",
    " 'Pre-Adoptive / Prospective Parents (PAP)',\n",
    " 'Teen Adoptee',\n",
    " 'Live, Love, Learn',\n",
    " 'transracial adoptee',\n",
    " 'adoptee // 23',\n",
    " 'TRA/ICA/KAD (minor)',\n",
    " 'Adult Adopteeü§ç',\n",
    " 'Adult adoptee, hoping to adopt',\n",
    " 'Punjabi-Canto transracial adoptee',\n",
    " 'Asian Adoptee',\n",
    " 'adoptee',\n",
    " 'Adoptee From USA',\n",
    " 'recently found my bio fam :)',\n",
    " 'non paternal event / LDA',\n",
    " 'Adopted',\n",
    " 'Late Disclosure Adoptee, Future Adoptive Parent',\n",
    " 'Private Infant Adoptee - 24F',\n",
    " 'adopted at infancy',\n",
    " '(Lifelong Open) Adoptee',\n",
    " 'Adoptee (üá®üá≥ ‚Äî&gt; üá´üá∑)',\n",
    " 'Late Discovery Adoptee (LDA)',\n",
    " 'International Asian TRA',\n",
    " \"int'l adoptee\",\n",
    " 'TRA',\n",
    " 'Teen Adoptee, open adoption',\n",
    " 'Adoptee &amp; Genealogical Detective!',\n",
    " 'adult adoptee',\n",
    " 'multiracial // transracial adoptee | prioritizing adoptee voices',\n",
    " 'step adoptee',\n",
    " 'half-adopted, hap',\n",
    " 'Adoptee @ 106 Days &amp; Genealogical Detective!',\n",
    " 'Adoptee; Confused as Hell',\n",
    " 'Second-generation adoptee',\n",
    " 'TRA/ICA',\n",
    " 'Adoptee (domestic infant adoption)',\n",
    " 'Closed Adoption Infant Adoptee',\n",
    " 'UK Adoptee',\n",
    " 'Closed domestic (US) infant adoptee in reunion',\n",
    " 'adopted & hap',\n",
    " 'Adopted in the late 60‚Äôs',\n",
    " 'Transracial Adult Adoptee',\n",
    " 'Adoptee @ 106 Days & Genealogical Detective',\n",
    " 'Adoptee & AP',\n",
    " 'adoptee & parent',\n",
    " 'Domestic Infant Adoptee',\n",
    " 'üá∑üá∫',\n",
    " 'Closed domestic (US) adult adoptee in reunion',\n",
    " 'Black adult invisible adoptee',\n",
    " 'Chinese American Adoptee',\n",
    " 'Adoptee (US)',\n",
    " '60s scoop reunited',\n",
    " 'transracial adoptee',\n",
    " 'DIA in Reunion',\n",
    " 'Adoptee of Closed Adoption',\n",
    " 'Private Infant Adoptee - 25F',\n",
    " 'transracial closed adoptee',\n",
    " \"adoptee '87\",\n",
    " 'TRA / Chinese adoptee',\n",
    " 'Late discovery adoptee, 26 yrs. Met bio families.',\n",
    " 'Adult DIA Adoptee',\n",
    " 'late age adoptee',\n",
    " 'adoptee & birthparent',\n",
    " 'victim of domestic & state violence via transracial adoption',\n",
    " 'Adoptee, Birthmother, & Parent',\n",
    " 'Domestic Adoptee 1988',\n",
    " 'BIA adoptee',\n",
    " 'foster care (2007-2010) / adopted (2010)',\n",
    " 'adoptee open adoption',\n",
    " 'Adoptee and Psychologist',\n",
    " 'Who am I?',\n",
    " 'domestic infant(ish) adoptee',\n",
    " 'Adoptee of Failed Adoption',\n",
    " 'Adoptee and  Birthmother',\n",
    " 'adoptee + adoptive parent',\n",
    " 'Reunited Adoptee &amp; Adoptee Rights Activist',\n",
    " 'Adoptee, Adoptive Parent',\n",
    " 'adoptee 3.11.87',\n",
    " 'Adopted Faery',\n",
    " 'Adopted Kid',\n",
    " 'Adopted from Bangladesh',\n",
    " 'KAD wutup!',\n",
    " 'Adopted: birth.  Found bio siblings: age 20.',\n",
    " 'Adoptee, 29F',\n",
    " 'Closed adoption: birth. Found bio siblings: age 20.',\n",
    " 'Adoptee /  Adoptive Parent',\n",
    " 'Adoptee Found Birth Family',\n",
    " 'adoptee; foster parent',\n",
    " 'Adopted/Plans to adopt',\n",
    " 'Transracial Adoptee (KAD)',\n",
    " 'Birthmom+Adoptee',\n",
    " 'Adopted at Birth | Found Birthfamily',\n",
    " 'Foster Parent/Adoptee',\n",
    " 'Moderator, adoptee',\n",
    " 'Two moms, two dads, lucky reunited adoptee',\n",
    " 'adoptee / plans to adopt',\n",
    " 'Adopted 1993 | Reunited 2017',\n",
    " 'Adopted as a baby',\n",
    " 'Chinese adoptee',\n",
    " 'Adoptee (International)',\n",
    " 'Adoptee, Birthmother, Adoptive parent',\n",
    " 'Adoptee, Activist',\n",
    " 'Adopted from Russia',\n",
    " 'Adoptee - Found birth family',\n",
    " 'Punjabi-Canto interracial adoptee',\n",
    " 'Korean adoptee',\n",
    " 'Transracial Adoptee &amp; Birth Mother',\n",
    " 'Trans-Racial Adoptee | PAP | Anti-Natalist',\n",
    " 'Kazakh adoptee',\n",
    " 'late-discovery adoptee, ex-ward',\n",
    " 'LDA, ex-ward, Indigenous post-ICWA adoptee',\n",
    " 'Adult Adoptee/Found Bio Parents - Ohio 1986 Prive Adoption',\n",
    " 'Transnational Adoptee from Birth',\n",
    " \"author, the adoptee's guide to dna testing (book)\",\n",
    " 'LDA, ex-ward, Indigenous adoptee',\n",
    " 'Adoptee, Only Child',\n",
    " 'Closed Adoption Adoptee Reunited',\n",
    " 'late-discovery-adoptee, ex-foster-kid',\n",
    " 'Adoptee &amp; Adopter',\n",
    " 'Taiwanese Adoptee',\n",
    " 'r/Adoptee Moderator',\n",
    " 'Closed DIA',\n",
    " 'Adopted aged four',\n",
    " 'Adult adoptee',\n",
    " 'Adopted @11days - reunited @ 27y/o',\n",
    " 'TRA/IA/LDA/AP/FP',\n",
    " 'intrafamily adoptee, school aged adoptee',\n",
    " 'Adoptee/closed Birthmom/open',\n",
    " 'china adoptee',\n",
    " 'Childhood adoptee/Birthmother to now adult',\n",
    " 'First Nations Adoptee',\n",
    " 'Chinese Transracial Adoptee',\n",
    " 'second-generation adoptee',\n",
    " 'International adoptee',\n",
    " 'International Transracial Adoptee',\n",
    " 'From Russia with Love?',\n",
    " 'FFY/Adoptee',\n",
    " 'TransAdoptedKid',\n",
    " 'adoptee and 23 ‚úåÔ∏è',\n",
    " 'Adopted Person',\n",
    " 'adoptee/former foster kid',\n",
    " 'Pre-Adoptive Parent | Adopted',\n",
    " 'adoptee // 24',\n",
    " 'victim of domestic &amp; state violence via transracial adoption',\n",
    " 'cambodian adoptee',\n",
    " 'Adult Adoptee Found BioFamily',\n",
    " 'Birth adoptee reunited w/BM &amp; Half-Siblings',\n",
    " 'Adoptee, may consider adoption in the future',\n",
    " 'Adopted from China',\n",
    " 'Adopted at birth',\n",
    " 'Transracial Indigenous Adoptee',\n",
    " 'Adoptee from birth',\n",
    " 'Adopted Chinese',\n",
    " 'reunited adoptee',\n",
    " 'TRA/ICA/KAD',\n",
    " 'Adopted as an Infant',\n",
    " 'Transracial/international Adoptee',\n",
    " 'open adoptee from birth',\n",
    " 'Half-adopted',\n",
    " 'adoptee &amp; parent',\n",
    " 'Adopted at Birth',\n",
    " 'domestic adoptee at birth | found birthparents']\n",
    "adoptee_labels = [re.escape(i) for i in adoptee_labels]\n",
    "adoptee_labels = \"|\".join(adoptee_labels)\n",
    "\n",
    "non_adoptee_labels = ['Former Foster Youth',\n",
    " 'Future AP',\n",
    " 'Bio Parent',\n",
    " 'Birthfather',\n",
    " 'Prospective Adoptive Parent',\n",
    " 'Birth Mother - Open Adoption',\n",
    " 'Mom through private domestic open transracial adoption',\n",
    " 'Reunited mother, former legal guardian, NPE',\n",
    " 'Reunited Mom, Foster Mom, L8 Dscvry Adoptee-paternal side',\n",
    " 'Reunited Birthparent.',\n",
    " 'Current Intl AP, Past Temp Foster Child',\n",
    " 'Birth Mom',\n",
    " 'AP, former FP, ASis',\n",
    " 'Birthmother.',\n",
    " 'birthmother',\n",
    " 'Birthmother 6/23/12',\n",
    " '14 adoptions in my family',\n",
    " 'Hopeful AP',\n",
    " 'Adoptive parent',\n",
    " 'birth parent',\n",
    " 'Adoptive Parent',\n",
    " 'Birthmother, 2002',\n",
    " 'Biological Father - searching',\n",
    " 'looking to adopt',\n",
    " 'adoptive father',\n",
    " 'Father of sibling group of 3',\n",
    " 'birth mom',\n",
    " 'Hopeful adopter',\n",
    " 'Adopting in Arkansas',\n",
    " 'Birthmom',\n",
    " 'Hoping to adopt',\n",
    " 'Looking into Adopting',\n",
    " 'caseyalexanderblog.wordpress.com',\n",
    " 'naturalmother_8-14-01',\n",
    " 'Someday-adopter',\n",
    " 'Potential Birthmother',\n",
    " 'waiting prospective AP',\n",
    " 'future AP',\n",
    " 'foster-to-adopt aunt/mom',\n",
    " 'Adoptive Dad',\n",
    " 'BP',\n",
    " 'I Fostered &amp; Then Adopted',\n",
    " 'Adoptive Parent/Orphanage Supervisor',\n",
    " 'Adoptive Parent (fostadopt)',\n",
    " 'hypervigilant.org',\n",
    " 'Possible AP',\n",
    " 'fost-adopt parent',\n",
    " 'pre-adoptive parent',\n",
    " 'Birth Parent',\n",
    " 'Birth Father &amp; /r/OpenAdoption Owner',\n",
    " 'RecentBM',\n",
    " 'Birthmother',\n",
    " 'Adoptive/Foster Mom',\n",
    " 'Adoptive Father',\n",
    " 'foster adopt',\n",
    " 'Adopted Family Member?',\n",
    " 'Prospective adoptive parent',\n",
    " 'Pre-fostering | prospective foster',\n",
    " 'First Mother',\n",
    " 'Bio of 2, Adoptive of 2',\n",
    " 'Sister adopted in x2, aunt adopted out x1',\n",
    " 'Hopeful APs',\n",
    " 'Researching foster/adoptive parenting',\n",
    " 'future FAD parent',\n",
    " 'Birthmother, Open Adoption',\n",
    " 'Adopting!',\n",
    " 'Adoptive Parent - Intercountry + Fostered',\n",
    " 'Post-Adoptions social worker', \n",
    " 'Adoptive Parent, Data Analyst',\n",
    " 'Bio sis',\n",
    " 'Adoptive/Foster Parent',\n",
    " 'ex-foster-kid',\n",
    " 'kinship adoptive parent / foster parent',\n",
    " 'Birthmother 2/13/2002',\n",
    " 'Transracial Adoptive Parent/Foster Parent',\n",
    " 'parent of adopted kids',\n",
    " 'Adoptive Mama',\n",
    " 'AP',\n",
    " 'Birthfather 10/21/1986',\n",
    " 'Interested, but no plans',\n",
    " 'Planning to Adopt in the Future',\n",
    " 'Future Foster/Adoptive Parent',\n",
    " 'Birth mother',\n",
    " 'Foster parent/Adoptive parent',\n",
    " 'Firstmom',\n",
    " 'Son, 12.. BirthMom',\n",
    " 'Father of 3, all adopted',\n",
    " 'kinship/foster parent',\n",
    " 'Momma',\n",
    " '23F- Future Adopter',\n",
    " 'Reunited Mom',\n",
    " 'Foster parent',\n",
    " 'Adoptive father',\n",
    " 'firstmother 2001',\n",
    " 'Adoptive mom of 3',\n",
    " 'Adoptive Dad of 3, soon 6',\n",
    " 'bio sibling',\n",
    " '5 failed matches, currently in #6 due winter 2016',\n",
    " 'Soon to be mom',\n",
    " 'Potential Adoptive Parent',\n",
    " 'foster/pre-adoptive parent',\n",
    " 'Intl Adoptive Parent',\n",
    " 'In Progress',\n",
    " 'Wanting to Adopt',\n",
    " 'Birth Father',\n",
    " '2 failed matches, still hoping',\n",
    " 'adoptive mom',\n",
    " '3 failed matches, still hoping',\n",
    " 'Someday-adopter, adoptive sister',\n",
    " 'Future adoptive parent',\n",
    " 'Matched with an expectant mom due in winter 2015',\n",
    " 'Adoptive Parents',\n",
    " 'Birthmother (Open Adoption)',\n",
    " 'AParent to teen',\n",
    " 'Son, 8.. BirthMom',\n",
    " 'Future Parent',\n",
    " 'considering adopting',\n",
    " 'Looking to adopt (Ontario)',\n",
    " 'Homestudied and waiting',\n",
    " 'Adopting thru Foster Care',\n",
    " 'sister of adoptee; future adoptive parent',\n",
    " 'Adoptive sister',\n",
    " 'Adoptive Dad of 3, soon 5',\n",
    " 'Adoption Researcher',\n",
    " 'was a foster parent',\n",
    " 'Parent',\n",
    " 'hoping to adopt',\n",
    " 'Father of 4 adopted sons',\n",
    " 'birthmom 2010, beautiful boy!',\n",
    " 'would like to adopt',\n",
    " 'Foster-to-Adopt',\n",
    " 'Birthmom 7/31/1992',\n",
    " 'potential adoptive father',\n",
    " '-25-groomer-wannabe adopter',\n",
    " 'Luckiest',\n",
    " 'Homestudied hopeful adopter',\n",
    " '3 adopted',\n",
    " 'sister of an adoptee',\n",
    " 'parent of several adopted kids',\n",
    " 'Birthmother, Daughter, Sister, Aunt, and Wife',\n",
    " 'adoptDad',\n",
    " '(b-mom, 1976)',\n",
    " 'Birthmom 3/15/98',\n",
    " 'Adoption Specialist',\n",
    " 'Fost-Adoptive parent of 3',\n",
    " 'HAP',\n",
    " 'Adoptive mom - open kinship',\n",
    " 'sister of adoptee; hopeful future AP',\n",
    " 'Birth Mom',\n",
    " 'haole, male, father to a daughter who was adopted, but not by me',\n",
    " 'may adopt in the future',\n",
    " 'LGBT adoptive parent &amp; daughter of adoptee',\n",
    " 'Adoptive Parent of Older Teen',\n",
    " 'prospective/pre-adoptive parent',\n",
    " 'Birthmom 2017',\n",
    " 'Birthmom 12/18/18',\n",
    " 'Furture adoptive mom, by choice.',\n",
    " 'Adoptive Parent x3',\n",
    " 'Future Father',\n",
    " 'Adoptive Parent &amp; Spouse to Adoptee',\n",
    " 'Adoptive Mama',\n",
    " 'prospective adoptive parent',\n",
    " 'Bio-Sis, Hopefully Future Adoptive Parent',\n",
    " 'Kinship AP',\n",
    " 'NY, Adoptive Parent, Permanency Specialist',\n",
    " 'Prospective AP',\n",
    " 'FFY - AP',\n",
    " 'Perspective adoptive parent',\n",
    " 'Adoptive Parent (International/Transracial)',\n",
    " 'Birth Mum.',\n",
    " 'Former foster kid. Almost-adopted more than once.',\n",
    " 'Adoptive Mother | Australia',\n",
    " 'Hoping to Adopt',\n",
    " 'Parent by Adoption',\n",
    " 'Researching PAP',\n",
    " 'One Adopted (Kinship), Seven Bio',\n",
    " 'Adoptive Parent (Kinship Via Husband)',\n",
    " 'foster mom',\n",
    " 'Younger Bio Sibling',\n",
    " 'Foster/Adoptive parent',\n",
    " 'AdoptiveParent',\n",
    " 'Adult Child of Adoptee',\n",
    " 'AP of teen',\n",
    " 'adoptive parent',\n",
    " 'Birth mom, 2017',\n",
    " 'Birth Mum of two - adopted by force.',\n",
    " 'buried under a pile of children',\n",
    " 'Adoptive mom',\n",
    " 'Daughter of 2 adoptees',\n",
    " 'Adoptive Mother',\n",
    " 'Reunited Birthparent.',\n",
    " 'Foster Parent',\n",
    " 'Adoptive Dad of 5 (2 sib grps from foster care)',\n",
    " 'Birthmother 12/13/2002',\n",
    " 'PAP',\n",
    " 'Prospective Parent',\n",
    " 'AP, former FP, ASis',\n",
    " 'biological parent',\n",
    " 'Birth Parent in StepParent Adoption',\n",
    " 'Open Adoption Birth Father &amp; /r/OpenAdoption Owner',\n",
    " 'Foster Parent, Child Welfare Public Health Professional',\n",
    " 'Have adopted-in siblings; searching for adopted-out sister',\n",
    " 'Pre-Placement Parent',\n",
    " '16|05.20.2020|Adoption',\n",
    " 'Prep-Adoptive',\n",
    " 'daughter of an adoptee',\n",
    " 'Adoptive Mom',\n",
    " 'reunited mom, lgl grdian, NPE',\n",
    " 'Birth Mother',\n",
    " 'NPE',\n",
    " 'Bio-Sib of an adoptee',\n",
    " 'Potential Foster Parent',\n",
    " 'Foster / Adoptive Parent',\n",
    " 'Foster Mom',\n",
    " 'mother was adopted',\n",
    " 'Stepmum to long lost adoptee / reunited',\n",
    " 'daughter of an adoptee. future adoptive parent.',\n",
    " 'Birthparent / Baby Girl due 12/28 :)',\n",
    " 'Prospective Adoptive Mother',\n",
    " 'Reunited Birthmom',\n",
    " 'Foster/Adoptive Parent',\n",
    " 'Hopeful Adoptive Parent',\n",
    " 'foster parent',\n",
    " 'Prospective Adoptive Parent',\n",
    " 'hopeful foster parent',\n",
    " '15 adoptions in my family',\n",
    " 'Reunited Birthparent.',\n",
    " 'Reunited Bio Mom',\n",
    " 'Adoptive Parent &amp; Adoptee‚Äôs spouse',\n",
    "]\n",
    "non_adoptee_labels = [re.escape(i) for i in non_adoptee_labels]\n",
    "non_adoptee_labels = \"|\".join(non_adoptee_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "adoptee_pattern = re.compile(r\"{}\".format(adoptee_labels), re.IGNORECASE)\n",
    "non_adoptee_pattern = re.compile(r\"{}\".format(non_adoptee_labels), re.IGNORECASE)\n",
    "\n",
    "def categorize_user(x):\n",
    "    if adoptee_pattern.match(x):\n",
    "        return 1  # Adoptee\n",
    "    elif non_adoptee_pattern.match(x):\n",
    "        return 0  # Non-adoptee\n",
    "    else:\n",
    "        return 2  # NEI\n",
    "\n",
    "all_df[\"is_adoptee\"] = all_df[\"user_flair\"].astype(str).apply(categorize_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/301967 [00:00<?, ?it/s]C:\\Users\\Ethan\\AppData\\Local\\Temp\\ipykernel_17684\\814632459.py:40: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  post_text = BeautifulSoup(post_text, 'lxml').get_text().strip() # re.sub(r'<.*?>', '', text)\n",
      "  1%|          | 1944/301967 [00:00<00:58, 5126.10it/s]C:\\Users\\Ethan\\AppData\\Local\\Temp\\ipykernel_17684\\814632459.py:40: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  post_text = BeautifulSoup(post_text, 'lxml').get_text().strip() # re.sub(r'<.*?>', '', text)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 301967/301967 [01:18<00:00, 3829.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# functions from TextClassification.ipynb MACS 30100\n",
    "\n",
    "from tqdm import tqdm\n",
    "import re, string\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def decontracted(phrase):\n",
    "    \"\"\"\n",
    "    Expand the contracted phrase into normal words\n",
    "    \"\"\"\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase) # prime \n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    \n",
    "    return phrase\n",
    "\n",
    "\n",
    "def clean_text(df):\n",
    "    \"\"\"\n",
    "    Clean the review texts\n",
    "    \"\"\"\n",
    "    cleaned_post_text = []\n",
    "\n",
    "    for post_text in tqdm(df['post_text']):\n",
    "        \n",
    "        # expand the contracted words\n",
    "        post_text = decontracted(post_text)\n",
    "        \n",
    "        #remove html tags\n",
    "        post_text = BeautifulSoup(post_text, 'lxml').get_text().strip() # re.sub(r'<.*?>', '', text)\n",
    "        \n",
    "        #remove non-alphabetic characters\n",
    "        post_text = re.sub(\"[^a-zA-Z]\",\" \", post_text)\n",
    "    \n",
    "        #remove url \n",
    "        post_text = re.sub(r'https?://\\S+|www\\.\\S+', '', post_text)\n",
    "        \n",
    "        #Removing punctutation, string.punctuation in python consists of !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~`\n",
    "        post_text = post_text.translate(str.maketrans('', '', string.punctuation))\n",
    "        # ''.join([char for char in movie_text_data if char not in string.punctuation])\n",
    "        \n",
    "        # remove emails\n",
    "        post_text = re.sub(r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\", '', post_text)\n",
    "    \n",
    "        cleaned_post_text.append(post_text)\n",
    "\n",
    "    return cleaned_post_text\n",
    "\n",
    "all_df['cleaned_post_text'] = clean_text(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_df = all_df[all_df.is_adoptee < 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.to_pickle(\"all_df.pkl\")\n",
    "truth_df.to_pickle(\"truth_df.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
